{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ebe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTA_STEPS 0, 1, and 2\n",
    "# Step 0: Data Cleaning\n",
    "# Step 1: Predict outcome\n",
    "# Step 2: Compute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5daaa557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import nltk\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from scipy.stats import sem\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d97264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57b07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4502f4",
   "metadata": {},
   "source": [
    "The dataset for the following example was also used by Roverts et al. (2014) to validate the Structural Topic Model (STM), and it is available on Harvard Dataverse. It is called \"gadarian_metadata.csv\" and is available under https://doi.org/10.7910/DVN/29405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf51dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>anger_ra1</th>\n",
       "      <th>anger_ra2</th>\n",
       "      <th>caseid</th>\n",
       "      <th>enthusiasm_ra1</th>\n",
       "      <th>enthusiasm_ra2</th>\n",
       "      <th>fear_ra1</th>\n",
       "      <th>fear_ra2</th>\n",
       "      <th>immigrants_ra1</th>\n",
       "      <th>immigrants_ra2</th>\n",
       "      <th>legal</th>\n",
       "      <th>open ended response</th>\n",
       "      <th>pid_rep</th>\n",
       "      <th>treat</th>\n",
       "      <th>txtorg_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>problems caused by the influx of illegal immig...</td>\n",
       "      <td>1</td>\n",
       "      <td>1. worried</td>\n",
       "      <td>27417620-757f-11e2-ad47-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>if you mean illegal immigration, i'm afraid of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1. worried</td>\n",
       "      <td>27475b94-757f-11e2-ad47-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>that they should enter the same way my grandpa...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0. think</td>\n",
       "      <td>27484838-757f-11e2-ad47-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>legally entering the usa meeting the requireme...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0. think</td>\n",
       "      <td>274a61fe-757f-11e2-ad47-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>terror\\nbombings\\nkilling us\\nrobbing america</td>\n",
       "      <td>0.66667002</td>\n",
       "      <td>1. worried</td>\n",
       "      <td>274c8592-757f-11e2-ad47-88532e617cea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name path anger_ra1 anger_ra2 caseid enthusiasm_ra1 enthusiasm_ra2 fear_ra1  \\\n",
       "0  NaN  NaN         0         0    287              0              0        1   \n",
       "1  NaN  NaN         0         0    145              0              0        1   \n",
       "2  NaN  NaN         1         1    159              0              0        0   \n",
       "3  NaN  NaN         0         1    421              0              0        1   \n",
       "4  NaN  NaN         1         0    224              0              0        2   \n",
       "\n",
       "  fear_ra2 immigrants_ra1 immigrants_ra2 legal  \\\n",
       "0        2              0              0     0   \n",
       "1        1              0              0     1   \n",
       "2        0              0              0     1   \n",
       "3        2              0              0     1   \n",
       "4        2              0              0     0   \n",
       "\n",
       "                                 open ended response     pid_rep       treat  \\\n",
       "0  problems caused by the influx of illegal immig...           1  1. worried   \n",
       "1  if you mean illegal immigration, i'm afraid of...           1  1. worried   \n",
       "2  that they should enter the same way my grandpa...       0.333    0. think   \n",
       "3  legally entering the usa meeting the requireme...         0.5    0. think   \n",
       "4      terror\\nbombings\\nkilling us\\nrobbing america  0.66667002  1. worried   \n",
       "\n",
       "                              txtorg_id  \n",
       "0  27417620-757f-11e2-ad47-88532e617cea  \n",
       "1  27475b94-757f-11e2-ad47-88532e617cea  \n",
       "2  27484838-757f-11e2-ad47-88532e617cea  \n",
       "3  274a61fe-757f-11e2-ad47-88532e617cea  \n",
       "4  274c8592-757f-11e2-ad47-88532e617cea  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path_data=\"\"\n",
    "df = pd.read_csv(path_data+\"gadarian_metadata.csv\",sep=\",\",dtype=str)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f2c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify text and ourcome\n",
    "text=df['open ended response']\n",
    "outcome=df['treat'].str[0]\n",
    "metadata=df[['anger_ra1','anger_ra2','enthusiasm_ra1','enthusiasm_ra2',\n",
    "             'fear_ra1','fear_ra2','immigrants_ra1','immigrants_ra2','legal','pid_rep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d15b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words_orig(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(str(sentence).split())\n",
    "\n",
    "\n",
    "# Load the spacy model (for other languages, use another spacy model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = text.apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop]))\n",
    "\n",
    "# lemmatize text (optional)\n",
    "\n",
    "text_lemma = text.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "df_tosave=pd.DataFrame({'text':text_lemma,'treatment':outcome, 'text_raw':text})\n",
    "df_tosave = pd.concat([df_tosave, metadata], axis=1)\n",
    "df_tosave = df_tosave[df_tosave['text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "text_lemma=df_tosave['text']\n",
    "outcome=df_tosave['treatment']\n",
    "text_orig=df_tosave['text_raw']\n",
    "\n",
    "\n",
    "words = text_lemma.apply(nltk.word_tokenize)\n",
    "words_orig = text_orig.apply(nltk.word_tokenize)\n",
    "\n",
    "# Flatten the list of words\n",
    "words = [word for sublist in words for word in sublist]\n",
    "words_orig = [word for sublist in words_orig for word in sublist]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist = nltk.FreqDist(words)\n",
    "\n",
    "sorted_freqdist = sorted(freqdist.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist_orig = nltk.FreqDist(words_orig)\n",
    "\n",
    "sorted_freqdist_orig = sorted(freqdist_orig.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Define minimum number of characters\n",
    "min_characters = 3\n",
    "\n",
    "# Assuming text_lemma is your lemmatized text\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w{%d,}\\b' % min_characters)\n",
    "\n",
    "# fit the vectorizer on the lemmatized text\n",
    "X = vectorizer.fit_transform(text_lemma)\n",
    "\n",
    "# X is a sparse matrix representing the bag-of-words model\n",
    "# To get the feature names (words), you can use\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# To convert the matrix into a DataFrame:\n",
    "# Here, feature_names (which are your words) are used as column names in the DataFrame\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "data_words = list(sent_to_words_orig(text_lemma))\n",
    "\n",
    "data_words_orig = list(sent_to_words_orig(text_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc00fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecdba918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: predict outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb2bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text and outcome for BERT\n",
    "\n",
    "text_for_bert=df_tosave['text']\n",
    "labels_for_bert=df_tosave['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d69a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n"
     ]
    }
   ],
   "source": [
    "# Predict outcome with BERT\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can use any seed you want\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Assume you have a DataFrame 'df' with 'text_lemma' as text column and 'outcome' as label\n",
    "texts = text_for_bert.tolist()\n",
    "labels = pd.to_numeric(labels_for_bert).tolist()\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(20):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions for each batch\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53dbb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: obtain feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to obtain feature importance\n",
    "\n",
    "# Define a prediction function, use CPU\n",
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=512, truncation=True) for v in x])\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# Explain the model's predictions on text\n",
    "shap_values = explainer(text_for_bert, fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify words that could be allocated to an embedding (important for Step 3)\n",
    "unique_words = list(set(words))\n",
    "unique_words_filtered = [word for word in unique_words if len(word) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059dc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(349, 1088)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe out of the SHAP values lists\n",
    "\n",
    "shap_allwords=pd.DataFrame(np.zeros((df_tosave.shape[0], len(unique_words_filtered))))\n",
    "shap_allwords.columns=unique_words_filtered\n",
    "\n",
    "index=0\n",
    "for i in range(text_lemma.shape[0]):\n",
    "    words_tmp=shap_values[i].data\n",
    "    shap_tmp=shap_values[i].values\n",
    "    for j in range(len(words_tmp)):\n",
    "        word_tmp=words_tmp[j].lstrip()\n",
    "        if word_tmp in unique_words_filtered:\n",
    "            shap_allwords[word_tmp][i]=shap_tmp[j]\n",
    "            index=index+1\n",
    "#             print(index)\n",
    "shap_allwords.shape      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60bba040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save SHAP values\n",
    "\n",
    "shap_allwords.to_csv(\"/shap_bert_allwords_010324_train_lemma_shap_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc585c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60825ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two alternative to BERT that are faster, but courl be less precise: Random Forest and OLS with Lasso (in both cases with BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=500, random_state=42,bootstrap=False,n_jobs=5)\n",
    "\n",
    "# Train the classifier on the whole sample\n",
    "clf.fit(bow_df, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322431f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TreeExplainer object\n",
    "explainer = shap.TreeExplainer(clf, feature_perturbation='interventional')\n",
    "\n",
    "# explainer = shap.TreeExplainer(clf, feature_perturbation='tree_path_dependent')\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(bow_df, check_additivity=False)\n",
    "\n",
    "shap_values_ones=shap_values[1]\n",
    "shap_values_matrix=[]\n",
    "for a in range(len(shap_values_ones)):\n",
    "    shap_values_matrix.append(shap_values_ones[a])\n",
    "\n",
    "\n",
    "shap_values_matrix=pd.DataFrame(shap_values_matrix)\n",
    "shap_values_matrix.columns=bow_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24f625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming bow_df is your DataFrame of features and outcome is your target variable\n",
    "X = bow_df.copy()  # Adding a constant (intercept term) to the features\n",
    "y = pd.to_numeric(outcome2)\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "# Fit the model\n",
    "# model = sm.Logit(y, X)\n",
    "model = sm.OLS(y, X)\n",
    "# model = sm.Probit(y, X)\n",
    "# result = model.fit()\n",
    "\n",
    "result_regularized = model.fit_regularized(method='elastic_net', alpha=1.0, L1_wt=1.0)\n",
    "\n",
    "# Refit the model using the parameters from the regularized fit\n",
    "model_refit = sm.OLS(y, X)\n",
    "result_refit = model_refit.fit(params=result_regularized.params)\n",
    "\n",
    "# Now you can print the summary\n",
    "print(result_refit.summary())\n",
    "\n",
    "# Print the summary\n",
    "# print(result.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the summary table from the model results\n",
    "summary_table = result_refit.summary().tables[1]\n",
    "\n",
    "# Convert the summary table to a DataFrame\n",
    "shap_mean = pd.DataFrame(summary_table.data)\n",
    "\n",
    "# Set the column names of the DataFrame\n",
    "shap_mean.columns = shap_mean.iloc[0]\n",
    "\n",
    "# Remove the first row (which contained the column names)\n",
    "shap_mean = shap_mean.iloc[1:]\n",
    "\n",
    "# Rename the columns to your preferred names\n",
    "shap_mean = shap_mean.rename(columns={\"\": \"Variable\", \"coef\": \"SHAP_mean\", \"t\": \"t_val\", \"P>|t|\": \"p-value\"})\n",
    "\n",
    "shap_mean[\"SHAP_mean\"] = pd.to_numeric(shap_mean[\"SHAP_mean\"])\n",
    "shap_mean[\"t_val\"] = pd.to_numeric(shap_mean[\"t_val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd6575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d3440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
