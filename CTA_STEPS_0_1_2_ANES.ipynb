{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ebe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTA_STEPS 0, 1, and 2\n",
    "# Step 0: Data Cleaning\n",
    "# Step 1: Predict outcome\n",
    "# Step 2: Compute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5daaa557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import nltk\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from scipy.stats import sem\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d97264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57b07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4502f4",
   "metadata": {},
   "source": [
    "The dataset for the following example was also used by Roberts et al. (2014) to validate the Structural Topic Model (STM), and it is available on Harvard Dataverse. It is called \"gadarian_metadata.csv\" and is available under https://doi.org/10.7910/DVN/29405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf51dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>age</th>\n",
       "      <th>caseid</th>\n",
       "      <th>female</th>\n",
       "      <th>highest grade completed</th>\n",
       "      <th>ideology</th>\n",
       "      <th>independents</th>\n",
       "      <th>mii_1</th>\n",
       "      <th>mii_2</th>\n",
       "      <th>...</th>\n",
       "      <th>mippol2_substantive3</th>\n",
       "      <th>mippol2_substantive4</th>\n",
       "      <th>mippol2_substantive5</th>\n",
       "      <th>mippol2_substantive6</th>\n",
       "      <th>mippol2_substantive7</th>\n",
       "      <th>mippol2_substantive8</th>\n",
       "      <th>pid</th>\n",
       "      <th>pid_strong</th>\n",
       "      <th>pid_summary</th>\n",
       "      <th>txtorg_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>the economy is first</td>\n",
       "      <td>the 2 wars</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3de40634-3386-11e3-a3e4-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>abortion</td>\n",
       "      <td>gay marriage</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3deba592-3386-11e3-a3e4-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>economy</td>\n",
       "      <td>more jobs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3dede46a-3386-11e3-a3e4-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>character//no</td>\n",
       "      <td>//experience//no</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3df03382-3386-11e3-a3e4-88532e617cea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>experience</td>\n",
       "      <td>a record of what the person has done, or actua...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>3df1d52a-3386-11e3-a3e4-88532e617cea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  name path age caseid female highest grade completed ideology independents  \\\n",
       "0  NaN  NaN  35      1      1                      13        1            1   \n",
       "1  NaN  NaN  39      3      2                      16        2           -1   \n",
       "2  NaN  NaN  50      4      1                      12        1            3   \n",
       "3  NaN  NaN  72      5      1                      16        1           -1   \n",
       "4  NaN  NaN  66      7      2                      16        2            1   \n",
       "\n",
       "                  mii_1                                              mii_2  \\\n",
       "0  the economy is first                                         the 2 wars   \n",
       "1              abortion                                       gay marriage   \n",
       "2               economy                                          more jobs   \n",
       "3         character//no                                   //experience//no   \n",
       "4            experience  a record of what the person has done, or actua...   \n",
       "\n",
       "   ... mippol2_substantive3 mippol2_substantive4 mippol2_substantive5  \\\n",
       "0  ...                  NaN                  NaN                  NaN   \n",
       "1  ...                  NaN                  NaN                  NaN   \n",
       "2  ...                  NaN                  NaN                  NaN   \n",
       "3  ...                  NaN                  NaN                  NaN   \n",
       "4  ...                  NaN                  NaN                  NaN   \n",
       "\n",
       "  mippol2_substantive6 mippol2_substantive7 mippol2_substantive8 pid  \\\n",
       "0                  NaN                  NaN                  NaN   3   \n",
       "1                  NaN                  NaN                  NaN   2   \n",
       "2                  NaN                  NaN                  NaN   5   \n",
       "3                  NaN                  NaN                  NaN   2   \n",
       "4                  NaN                  NaN                  NaN   3   \n",
       "\n",
       "  pid_strong pid_summary                             txtorg_id  \n",
       "0         -1           4  3de40634-3386-11e3-a3e4-88532e617cea  \n",
       "1          5           5  3deba592-3386-11e3-a3e4-88532e617cea  \n",
       "2         -1           3  3dede46a-3386-11e3-a3e4-88532e617cea  \n",
       "3          1           6  3df03382-3386-11e3-a3e4-88532e617cea  \n",
       "4         -1           4  3df1d52a-3386-11e3-a3e4-88532e617cea  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path_data=\"\"\n",
    "df = pd.read_csv(path_data+\"final_anes_metadata.csv\",sep=\",\",dtype=str)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f2c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with ideology (-4)\n",
    "# identify text and ourcome\n",
    "df['ideology']=pd.to_numeric(df['ideology'])\n",
    "df['pid_summary']=pd.to_numeric(df['pid_summary'])\n",
    "df=df[df['pid_summary']>0]\n",
    "df=df[df['pid_summary']!=3].reset_index(drop=True)\n",
    "df['republican']=[1 if x>3 else 0 for x in df['pid_summary']]\n",
    "#text=df['mii_1']\n",
    "text=df['mip_1']\n",
    "#outcome=df['ideology']-1\n",
    "outcome=df['republican']\n",
    "text_original=df['mip_1']\n",
    "metadata=df[['miiele1_code1','mippol1_code1','age','female']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d15b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words_orig(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(str(sentence).split())\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "# Load the spacy model (for other languages, use another spacy model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = text.apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop]))\n",
    "\n",
    "# lemmatize text (optional)\n",
    "\n",
    "text_lemma = text.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "df_tosave=pd.DataFrame({'text':text_lemma,'republican':outcome, 'text_raw':text, 'text_original':text_original})\n",
    "df_tosave = pd.concat([df_tosave, metadata], axis=1)\n",
    "df_tosave = df_tosave[df_tosave['text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "text_lemma=df_tosave['text']\n",
    "outcome=df_tosave['republican']\n",
    "text_orig=df_tosave['text_raw']\n",
    "\n",
    "\n",
    "words = text_lemma.apply(nltk.word_tokenize)\n",
    "words_orig = text_orig.apply(nltk.word_tokenize)\n",
    "\n",
    "# Flatten the list of words\n",
    "words = [word for sublist in words for word in sublist]\n",
    "words_orig = [word for sublist in words_orig for word in sublist]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist = nltk.FreqDist(words)\n",
    "\n",
    "sorted_freqdist = sorted(freqdist.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist_orig = nltk.FreqDist(words_orig)\n",
    "\n",
    "sorted_freqdist_orig = sorted(freqdist_orig.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Define minimum number of characters\n",
    "min_characters = 3\n",
    "\n",
    "# Assuming text_lemma is your lemmatized text\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w{%d,}\\b' % min_characters)\n",
    "\n",
    "# fit the vectorizer on the lemmatized text\n",
    "X = vectorizer.fit_transform(text_lemma)\n",
    "\n",
    "# X is a sparse matrix representing the bag-of-words model\n",
    "# To get the feature names (words), you can use\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# To convert the matrix into a DataFrame:\n",
    "# Here, feature_names (which are your words) are used as column names in the DataFrame\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "data_words = list(sent_to_words_orig(text_lemma))\n",
    "\n",
    "data_words_orig = list(sent_to_words_orig(text_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdb79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35a48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecdba918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: predict outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb2bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text and outcome for BERT\n",
    "\n",
    "text_for_bert=df_tosave['text']\n",
    "labels_for_bert=df_tosave['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d69a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n"
     ]
    }
   ],
   "source": [
    "# Predict outcome with BERT\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can use any seed you want\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Assume you have a DataFrame 'df' with 'text_lemma' as text column and 'outcome' as label\n",
    "texts = text_for_bert.tolist()\n",
    "labels = pd.to_numeric(labels_for_bert).tolist()\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(20):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions for each batch\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53dbb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: obtain feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to obtain feature importance\n",
    "\n",
    "# Define a prediction function, use CPU\n",
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=512, truncation=True) for v in x])\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# Explain the model's predictions on text\n",
    "shap_values = explainer(text_for_bert, fixed_context=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb4e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify words that could be allocated to an embedding (important for Step 3)\n",
    "unique_words = list(set(words))\n",
    "unique_words_filtered = [word for word in unique_words if len(word) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "059dc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1297, 643)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe out of the SHAP values lists\n",
    "\n",
    "shap_allwords=pd.DataFrame(np.zeros((df_tosave.shape[0], len(unique_words_filtered))))\n",
    "shap_allwords.columns=unique_words_filtered\n",
    "\n",
    "index=0\n",
    "for i in range(text_lemma.shape[0]):\n",
    "    words_tmp=shap_values[i].data\n",
    "    shap_tmp=shap_values[i].values\n",
    "    for j in range(len(words_tmp)):\n",
    "        word_tmp=words_tmp[j].lstrip()\n",
    "        if word_tmp in unique_words_filtered:\n",
    "            shap_allwords[word_tmp][i]=shap_tmp[j]\n",
    "            index=index+1\n",
    "#             print(index)\n",
    "shap_allwords.shape      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60bba040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save SHAP values\n",
    "\n",
    "shap_allwords.to_csv(\"anes_shap_bert_allwords_110324_train_lemma_shap_lemma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8961f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
