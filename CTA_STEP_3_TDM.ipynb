{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79a26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTA_STEP 3 (Public goods game data)\n",
    "# Allocate pos/neg words to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be9649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import nltk\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from scipy.stats import sem\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7790412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprosessing is necessary because some steps make reference to the text data\n",
    "# Step 0: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003aca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19fc8856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>age</th>\n",
       "      <th>belief.in.god</th>\n",
       "      <th>condition_number</th>\n",
       "      <th>contribution</th>\n",
       "      <th>correct.crt.answers</th>\n",
       "      <th>country.of.residence..232.us.</th>\n",
       "      <th>datelastaction</th>\n",
       "      <th>...</th>\n",
       "      <th>social.risk.taking</th>\n",
       "      <th>strategy.description</th>\n",
       "      <th>time.delay</th>\n",
       "      <th>time.pressure</th>\n",
       "      <th>time.reading.instructions</th>\n",
       "      <th>time_pred</th>\n",
       "      <th>txtorg_id</th>\n",
       "      <th>us.resident</th>\n",
       "      <th>what.cont.maximizes.group.payoff</th>\n",
       "      <th>what.cont.maximizes.your.payoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>4/26/12 3:56</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>basically, given the choices of how much to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>na</td>\n",
       "      <td>4cf9192e-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>4/27/12 23:15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>i felt like it was the right decision. i had a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfa2db4-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>4/26/12 22:48</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>don't believe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfaf9ec-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>4/24/12 12:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>main reason for my contribution was the the fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfb1634-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>4/27/12 22:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>because i want to take a risk.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfb3dd0-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  name path Unnamed: 2 age belief.in.god condition_number contribution  \\\n",
       "0  NaN  NaN        154  22             1               10           40   \n",
       "1  NaN  NaN        155  24             1               10           30   \n",
       "2  NaN  NaN        157  25             1               10           40   \n",
       "3  NaN  NaN        158  24             9               10           40   \n",
       "4  NaN  NaN        159  26            10               10            0   \n",
       "\n",
       "  correct.crt.answers country.of.residence..232.us. datelastaction  ...  \\\n",
       "0                   3                           232   4/26/12 3:56  ...   \n",
       "1                   0                           232  4/27/12 23:15  ...   \n",
       "2                   3                           232  4/26/12 22:48  ...   \n",
       "3                   1                           203  4/24/12 12:39  ...   \n",
       "4                   2                           129  4/27/12 22:51  ...   \n",
       "\n",
       "  social.risk.taking                               strategy.description  \\\n",
       "0                3.6  basically, given the choices of how much to co...   \n",
       "1                3.2  i felt like it was the right decision. i had a...   \n",
       "2                3.4                                      don't believe   \n",
       "3                2.9  main reason for my contribution was the the fe...   \n",
       "4                2.1                     because i want to take a risk.   \n",
       "\n",
       "  time.delay time.pressure time.reading.instructions time_pred  \\\n",
       "0          0             0                        71        na   \n",
       "1          0             0                        72        na   \n",
       "2          0             0                        36        na   \n",
       "3          0             0                       104        na   \n",
       "4          0             0                       165        na   \n",
       "\n",
       "                              txtorg_id us.resident  \\\n",
       "0  4cf9192e-7591-11e2-89cc-88532e617cea           1   \n",
       "1  4cfa2db4-7591-11e2-89cc-88532e617cea           1   \n",
       "2  4cfaf9ec-7591-11e2-89cc-88532e617cea           1   \n",
       "3  4cfb1634-7591-11e2-89cc-88532e617cea           0   \n",
       "4  4cfb3dd0-7591-11e2-89cc-88532e617cea           0   \n",
       "\n",
       "  what.cont.maximizes.group.payoff what.cont.maximizes.your.payoff  \n",
       "0                               40                              40  \n",
       "1                                0                               0  \n",
       "2                               40                              40  \n",
       "3                               40                               0  \n",
       "4                               40                               0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path_data=\"\"\n",
    "path_graphs=\"\"\n",
    "\n",
    "df = pd.read_csv(path_data+\"tdm1011_metadata.csv\",sep=\",\",dtype=str)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4411ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify text and ourcome\n",
    "df['treatment']=pd.to_numeric(df['primed.for.intuition'])\n",
    "text=df['strategy.description']\n",
    "outcome=df['treatment']\n",
    "text_original=df['strategy.description']\n",
    "metadata=df[['education','female','contribution','normalized.contribution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec078277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words_orig(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(str(sentence).split())\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "# Load the spacy model (for other languages, use another spacy model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = text.apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop]))\n",
    "\n",
    "# lemmatize text (optional)\n",
    "\n",
    "text_lemma = text.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "df_tosave=pd.DataFrame({'text':text_lemma,'treatment':outcome, 'text_raw':text, 'text_original':text_original})\n",
    "df_tosave = pd.concat([df_tosave, metadata], axis=1)\n",
    "df_tosave = df_tosave[df_tosave['text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "text_lemma=df_tosave['text']\n",
    "outcome=df_tosave['treatment']\n",
    "text_orig=df_tosave['text_raw']\n",
    "\n",
    "\n",
    "words = text_lemma.apply(nltk.word_tokenize)\n",
    "words_orig = text_orig.apply(nltk.word_tokenize)\n",
    "\n",
    "# Flatten the list of words\n",
    "words = [word for sublist in words for word in sublist]\n",
    "words_orig = [word for sublist in words_orig for word in sublist]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist = nltk.FreqDist(words)\n",
    "\n",
    "sorted_freqdist = sorted(freqdist.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist_orig = nltk.FreqDist(words_orig)\n",
    "\n",
    "sorted_freqdist_orig = sorted(freqdist_orig.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Define minimum number of characters\n",
    "min_characters = 3\n",
    "\n",
    "# Assuming text_lemma is your lemmatized text\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w{%d,}\\b' % min_characters)\n",
    "\n",
    "# fit the vectorizer on the lemmatized text\n",
    "X = vectorizer.fit_transform(text_lemma)\n",
    "\n",
    "# X is a sparse matrix representing the bag-of-words model\n",
    "# To get the feature names (words), you can use\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# To convert the matrix into a DataFrame:\n",
    "# Here, feature_names (which are your words) are used as column names in the DataFrame\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "data_words = list(sent_to_words_orig(text_lemma))\n",
    "\n",
    "data_words_orig = list(sent_to_words_orig(text_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b9a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tosave.to_csv(path_data+'tdm_lemmatized.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29265b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2e4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions necessary for Step 3\n",
    "\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cosine_similarity\n",
    "\n",
    "def create_cosine_matrix(pretrained_model, word_list):\n",
    "    vocab_index = pd.DataFrame({'feature_names':word_list,'feature_index':range(1,len(word_list)+1)})\n",
    "    all_words_dict = {a: x-1 for a, x in vocab_index.values}\n",
    "    word_index = all_words_dict\n",
    "    EMBED_DIM = 300\n",
    "    words_not_found = []\n",
    "    nb_words = len(word_index)\n",
    "    embedding_matrix = np.zeros((nb_words, EMBED_DIM))\n",
    "    emb_name = []\n",
    "    index = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = pretrained_model.wv[word]\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            emb_name.append(word)\n",
    "            index = index + 1\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    if words_not_found != []:\n",
    "        print(f\"Null word embeddings: {np.sum(np.sum(embedding_matrix, axis=1) == 0)}\")\n",
    "        print(f\"Some of the words not found:\\n{' '.join([random.choice(words_not_found) for x in range(0,10)])}\")\n",
    "    n_words_notfound = embedding_matrix.shape[0] - len(emb_name)\n",
    "    embedding_matrix = np.delete(embedding_matrix, range(embedding_matrix.shape[0] - len(words_not_found), embedding_matrix.shape[0]), 0)\n",
    "    embedding_matrix_names = pd.DataFrame(np.transpose(embedding_matrix))\n",
    "    embedding_matrix_names.columns = emb_name\n",
    "    cosine_matrix = pd.DataFrame(np.zeros((len(emb_name), len(emb_name))), columns=emb_name, index=emb_name)\n",
    "    for i in range(cosine_matrix.shape[1]):\n",
    "        word_1 = cosine_matrix.columns[i]\n",
    "        for j in range(cosine_matrix.shape[1]):\n",
    "            word_2 = cosine_matrix.index[j]\n",
    "            cosine_matrix.iloc[j, i] = similarity_cosine(embedding_matrix_names[word_1], embedding_matrix_names[word_2])\n",
    "    return cosine_matrix\n",
    "\n",
    "\n",
    "\n",
    "def calculate_quality_all(words_per_topic, cosine_matrix):\n",
    "    quality_all_tmp = []\n",
    "    topic_combinations = list(combinations(range(len(words_per_topic)), 2))\n",
    "    for i in range(len(topic_combinations)):\n",
    "        topic_k = topic_combinations[i][0]\n",
    "        topic_l = topic_combinations[i][1]\n",
    "        topic_k_words = words_per_topic[topic_k]\n",
    "        topic_k_words_combinations = list(combinations(topic_k_words, 2))\n",
    "        topic_l_words = words_per_topic[topic_l]\n",
    "        topic_l_words_combinations = list(combinations(topic_l_words, 2))\n",
    "\n",
    "        # coherence_k\n",
    "        tmp_intra_topic_k = list(combinations(topic_k_words, 2))\n",
    "        coherence_k = []\n",
    "        for j in range(len(tmp_intra_topic_k)):\n",
    "            coherence_tmp = cosine_matrix.loc[topic_k_words_combinations[j][1], topic_k_words_combinations[j][0]]\n",
    "            coherence_k.append(coherence_tmp)\n",
    "        coherence_k = np.mean(coherence_k)\n",
    "\n",
    "        # coherence_l\n",
    "        tmp_intra_topic_l = list(combinations(topic_l_words, 2))\n",
    "        coherence_l = []\n",
    "        for j in range(len(tmp_intra_topic_l)):\n",
    "            coherence_tmp = cosine_matrix.loc[topic_l_words_combinations[j][1], topic_l_words_combinations[j][0]]\n",
    "            coherence_l.append(coherence_tmp)\n",
    "        coherence_l = np.mean(coherence_l)\n",
    "\n",
    "        # diversity k_l\n",
    "        topic_k_l_combinations = list(product(topic_k_words, topic_l_words))\n",
    "        diversity_k_l_combs = []\n",
    "        for k in range(len(topic_k_l_combinations)):\n",
    "            diversity_k_l_tmp = cosine_matrix.loc[topic_k_l_combinations[k][1], topic_k_l_combinations[k][0]]\n",
    "            diversity_k_l_combs.append(diversity_k_l_tmp)\n",
    "        diversity_k_l = 1 - np.mean(diversity_k_l_combs)\n",
    "\n",
    "        quality_k_l = (coherence_k + coherence_l) / 2 * diversity_k_l\n",
    "        quality_all_tmp.append(quality_k_l)\n",
    "\n",
    "    if len(quality_all_tmp) > 0:\n",
    "        quality_all = np.mean(quality_all_tmp)\n",
    "    else:\n",
    "        quality_all = 0\n",
    "\n",
    "    return quality_all\n",
    "\n",
    "\n",
    "def optimize_clusters(grid_alpha, grid_delta, word_importance, cosine_matrix):\n",
    "    median_distance = np.median(cosine_matrix)\n",
    "    interpretability_matrix = np.zeros((len(grid_delta), len(grid_alpha)))\n",
    "    index_m = 0\n",
    "    for m in grid_delta:\n",
    "        delta = m\n",
    "        index_n = 0\n",
    "        for n in grid_alpha:\n",
    "            min_distance = median_distance * n\n",
    "            words_to_drop = []\n",
    "            cluster_list_groups = []\n",
    "            mean_shap = []\n",
    "            impact_shap = []\n",
    "            word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "            while len(word_importance_clustering) > 1:\n",
    "                word_start = word_importance_clustering['Variable'].loc[0]\n",
    "                cosine_matrix_tmp = cosine_matrix.copy()\n",
    "                cosine_matrix_tmp.drop(words_to_drop, inplace=True, axis=0)\n",
    "                cosine_matrix_tmp.drop(words_to_drop, inplace=True, axis=1)\n",
    "                word_closest = cosine_matrix_tmp[word_start].nlargest(2).index[1]\n",
    "                dist_init = cosine_matrix_tmp[word_start].nlargest(2)[1]\n",
    "                if dist_init > min_distance:\n",
    "                    mean_dist_words = (cosine_matrix_tmp[word_start] + cosine_matrix_tmp[word_closest]) / 2\n",
    "                    dist_threshold = dist_init * delta\n",
    "                    cluster_list_tmp = [word_start, word_closest]\n",
    "                    dist_words = 1\n",
    "                    while (dist_words > dist_threshold) & (len(cosine_matrix_tmp) > len(cluster_list_tmp)):\n",
    "                        mean_dist_words = cosine_matrix_tmp[cluster_list_tmp].mean(axis=1)\n",
    "                        mean_dist_words.drop(cluster_list_tmp, inplace=True)\n",
    "                        word_add = mean_dist_words.nlargest(2).index[0]\n",
    "                        dist_words = mean_dist_words.nlargest(2)[0]\n",
    "                        cluster_list_tmp.append(word_add)\n",
    "                    words_to_drop.extend(cluster_list_tmp)\n",
    "                    cluster_list_groups.append(cluster_list_tmp)\n",
    "                    word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "                else:\n",
    "                    words_to_drop.extend([word_start])\n",
    "                    word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "            quality_all = calculate_quality_all(cluster_list_groups, cosine_matrix)\n",
    "            log_nwords = np.log(len([item for sublist in cluster_list_groups for item in sublist]) + 1)\n",
    "            interpretability = log_nwords * quality_all\n",
    "            interpretability_matrix[index_m, index_n] = interpretability\n",
    "            index_n = index_n + 1\n",
    "        index_m = index_m + 1\n",
    "    ind = np.unravel_index(np.argmax(interpretability_matrix, axis=None), interpretability_matrix.shape)\n",
    "    delta = grid_delta[ind[0]]\n",
    "    alpha = grid_alpha[ind[1]]\n",
    "    return alpha, delta\n",
    "\n",
    "\n",
    "\n",
    "def calculate_clusters(method, n_clusters, delta, alpha, cosine_matrix, word_importance):\n",
    "    words_to_drop = []\n",
    "    cluster_list_groups = []\n",
    "    mean_shap = []\n",
    "    impact_shap = []\n",
    "    mean_dist_words_topics = []\n",
    "\n",
    "    median_distance = np.median(cosine_matrix)\n",
    "    min_distance = alpha * median_distance\n",
    "    word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "\n",
    "    while len(word_importance_clustering) > 1:\n",
    "        word_start = word_importance_clustering['Variable'].loc[0]\n",
    "\n",
    "        cosine_matrix_tmp = cosine_matrix.copy()\n",
    "        cosine_matrix_tmp.drop(words_to_drop, inplace=True, axis=0)\n",
    "        cosine_matrix_tmp.drop(words_to_drop, inplace=True, axis=1)\n",
    "\n",
    "        word_closest = cosine_matrix_tmp[word_start].nlargest(2).index[1]\n",
    "        dist_init = cosine_matrix_tmp[word_start].nlargest(2)[1]\n",
    "        if dist_init > min_distance:\n",
    "            mean_dist_words = (cosine_matrix_tmp[word_start] + cosine_matrix_tmp[word_closest]) / 2\n",
    "\n",
    "            dist_threshold = dist_init * delta\n",
    "\n",
    "            cluster_list_tmp = [word_start, word_closest]\n",
    "\n",
    "            dist_words = 1\n",
    "            while (dist_words > dist_threshold) & (len(cosine_matrix_tmp) > len(cluster_list_tmp)):\n",
    "                mean_dist_words = cosine_matrix_tmp[cluster_list_tmp].mean(axis=1)\n",
    "                mean_dist_words.drop(cluster_list_tmp, inplace=True)\n",
    "                word_add = mean_dist_words.nlargest(2).index[0]\n",
    "                dist_words = mean_dist_words.nlargest(2)[0]\n",
    "                cluster_list_tmp.append(word_add)\n",
    "            words_to_drop.extend(cluster_list_tmp)\n",
    "            cluster_list_groups.append(cluster_list_tmp)\n",
    "            mean_dist_words_topics_tmp=np.mean(cosine_matrix_tmp[cluster_list_tmp])\n",
    "            if method != 'freq_only':\n",
    "                mean_dist_words_topics.append(mean_dist_words_topics_tmp)\n",
    "                mean_shap.append(word_importance[word_importance['Variable'].isin(cluster_list_tmp)]['word_importance_abs'].mean())\n",
    "                impact_shap.append(word_importance[word_importance['Variable'].isin(cluster_list_tmp)]['impact'].mean())\n",
    "            word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "        else:\n",
    "            words_to_drop.extend([word_start])\n",
    "            word_importance_clustering = word_importance[word_importance['Variable'].isin(words_to_drop) == False].reset_index(drop=True)\n",
    "\n",
    "    n_cluster_index = len(cluster_list_groups)\n",
    "    if n_cluster_index < n_clusters:\n",
    "        n_clusters_to_use = n_cluster_index\n",
    "    else:\n",
    "        n_clusters_to_use = n_clusters\n",
    "\n",
    "    if method == 'freq_only':\n",
    "        index_clusters = range(n_clusters_to_use)\n",
    "    elif method == 'top_shap':\n",
    "        index_clusters = range(n_clusters_to_use)\n",
    "    elif method == 'mean_shap':\n",
    "        index_clusters = np.argpartition(mean_shap, -n_clusters_to_use)[-n_clusters_to_use:]\n",
    "    elif method == 'impact':\n",
    "        index_clusters = np.argpartition(impact_shap, -n_clusters_to_use)[-n_clusters_to_use:]\n",
    "    elif method == 'coherence':\n",
    "        index_clusters = np.argpartition(mean_dist_words_topics, -n_clusters_to_use)[-n_clusters_to_use:]\n",
    "\n",
    "    cluster_list_groups = [cluster_list_groups[l] for l in index_clusters]\n",
    "\n",
    "\n",
    "    return cluster_list_groups, n_cluster_index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6e49eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 2154)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create word embeddings for clustering\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = FastText.load_fasttext_format(\"cc.en.300.bin\")\n",
    "\n",
    "# Update the model with new data\n",
    "\n",
    "pretrained_model.build_vocab(corpus_iterable=data_words, update=True)\n",
    "pretrained_model.train(corpus_iterable=data_words, total_examples=len(data_words), epochs=pretrained_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68f7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SHAP values\n",
    "\n",
    "shap_allwords=pd.read_csv(\"tdm_intuition_shap_bert_allwords_220324_train_lemma_shap_lemma.csv\")\n",
    "\n",
    "# copy to new dataframe \n",
    "shap_values_matrix=shap_allwords.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4157ce9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cooperate</th>\n",
       "      <th>rational</th>\n",
       "      <th>greedy</th>\n",
       "      <th>produce</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>mind</th>\n",
       "      <th>way</th>\n",
       "      <th>polite</th>\n",
       "      <th>cour</th>\n",
       "      <th>...</th>\n",
       "      <th>logical</th>\n",
       "      <th>interact</th>\n",
       "      <th>admit</th>\n",
       "      <th>outcome</th>\n",
       "      <th>safe</th>\n",
       "      <th>i.e</th>\n",
       "      <th>happy</th>\n",
       "      <th>retu</th>\n",
       "      <th>sacrificing</th>\n",
       "      <th>atheist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-0.123708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows Ã— 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        group  cooperate  rational  greedy  produce  intelligent  mind  \\\n",
       "0    0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "1    0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "2    0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "3    0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "4    0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "..        ...        ...       ...     ...      ...          ...   ...   \n",
       "243 -0.123708        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "244  0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "245  0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "246  0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "247  0.000000        0.0       0.0     0.0      0.0          0.0   0.0   \n",
       "\n",
       "          way  polite  cour  ...  logical  interact  admit  outcome  safe  \\\n",
       "0    0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "1    0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "2    0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "3    0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "4    0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "..        ...     ...   ...  ...      ...       ...    ...      ...   ...   \n",
       "243  0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "244 -0.009692     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "245  0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "246  0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "247  0.000000     0.0   0.0  ...      0.0       0.0    0.0      0.0   0.0   \n",
       "\n",
       "     i.e  happy  retu  sacrificing  atheist  \n",
       "0    0.0    0.0   0.0          0.0      0.0  \n",
       "1    0.0    0.0   0.0          0.0      0.0  \n",
       "2    0.0    0.0   0.0          0.0      0.0  \n",
       "3    0.0    0.0   0.0          0.0      0.0  \n",
       "4    0.0    0.0   0.0          0.0      0.0  \n",
       "..   ...    ...   ...          ...      ...  \n",
       "243  0.0    0.0   0.0          0.0      0.0  \n",
       "244  0.0    0.0   0.0          0.0      0.0  \n",
       "245  0.0    0.0   0.0          0.0      0.0  \n",
       "246  0.0    0.0   0.0          0.0      0.0  \n",
       "247  0.0    0.0   0.0          0.0      0.0  \n",
       "\n",
       "[248 rows x 444 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce29eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate shap mean value and perform t-test\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Variable': shap_values_matrix.columns,\n",
    "    'word_importance': shap_values_matrix.mean(),\n",
    "    'word_se': sem(shap_values_matrix),\n",
    "    'word_tval': [stats.ttest_1samp(shap_values_matrix[col], 0)[0] for col in shap_values_matrix.columns],\n",
    "    'word_pval': [stats.ttest_1samp(shap_values_matrix[col], 0)[1] for col in shap_values_matrix.columns]\n",
    "}).reset_index(drop=True)\n",
    "feature_importance = feature_importance.fillna(0)\n",
    "\n",
    "\n",
    "feature_importance['word_importance_abs']=abs(feature_importance['word_importance'])\n",
    "\n",
    "freqlist=[]\n",
    "for i in feature_importance['Variable']:\n",
    "    if len([item for item in sorted_freqdist if item[0] == i])>0:\n",
    "        freqlist.append([item for item in sorted_freqdist if item[0] == i][0][1])\n",
    "    else:\n",
    "        freqlist.append(0)\n",
    "        \n",
    "feature_importance['freq']=freqlist\n",
    "feature_importance['impact']=feature_importance['freq']*feature_importance['word_importance_abs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa44ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create descending list with top words for pos/neg, with p-value for SHAP>0 below 0.1\n",
    "feature_importance_pos=feature_importance[(feature_importance['word_pval']<0.1)&\n",
    "                                          (feature_importance['word_importance']>0)].reset_index(drop=True)\n",
    "\n",
    "feature_importance_neg=feature_importance[(feature_importance['word_pval']<0.1)&\n",
    "                                          (feature_importance['word_importance']<0)].reset_index(drop=True)\n",
    "\n",
    "feature_importance_pos=feature_importance_pos.sort_values('word_importance_abs',ascending=False).reset_index(drop=True)\n",
    "feature_importance_neg=feature_importance_neg.sort_values('word_importance_abs',ascending=False).reset_index(drop=True)\n",
    "\n",
    "#feature_importance_pos=feature_importance_pos.sort_values('word_pval',ascending=True).reset_index(drop=True)\n",
    "#feature_importance_neg=feature_importance_neg.sort_values('word_pval',ascending=True).reset_index(drop=True)\n",
    "#feature_importance_pos=feature_importance_pos.sort_values('impact',ascending=False).reset_index(drop=True)\n",
    "#feature_importance_neg=feature_importance_neg.sort_values('impact',ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b36fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>word_importance</th>\n",
       "      <th>word_se</th>\n",
       "      <th>word_tval</th>\n",
       "      <th>word_pval</th>\n",
       "      <th>word_importance_abs</th>\n",
       "      <th>freq</th>\n",
       "      <th>impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>4.701949</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>47</td>\n",
       "      <td>0.299325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>4.011916</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>25</td>\n",
       "      <td>0.144042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hope</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>4.627092</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.005610</td>\n",
       "      <td>25</td>\n",
       "      <td>0.140248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bonus</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>2.784304</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>14</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trust</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>2.072907</td>\n",
       "      <td>0.039218</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gamble</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>2.290893</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>god</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>1.977533</td>\n",
       "      <td>0.049093</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>self</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>2.145682</td>\n",
       "      <td>0.032873</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>well</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>1.651641</td>\n",
       "      <td>0.099879</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>play</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>1.905138</td>\n",
       "      <td>0.057923</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable  word_importance   word_se  word_tval  word_pval  \\\n",
       "0     good         0.006369  0.001354   4.701949   0.000004   \n",
       "1     feel         0.005762  0.001436   4.011916   0.000080   \n",
       "2     hope         0.005610  0.001212   4.627092   0.000006   \n",
       "3    bonus         0.001304  0.000468   2.784304   0.005780   \n",
       "4    trust         0.001247  0.000602   2.072907   0.039218   \n",
       "5   gamble         0.001159  0.000506   2.290893   0.022812   \n",
       "6      god         0.000846  0.000428   1.977533   0.049093   \n",
       "7     self         0.000550  0.000256   2.145682   0.032873   \n",
       "8     well         0.000504  0.000305   1.651641   0.099879   \n",
       "9     play         0.000180  0.000094   1.905138   0.057923   \n",
       "\n",
       "   word_importance_abs  freq    impact  \n",
       "0             0.006369    47  0.299325  \n",
       "1             0.005762    25  0.144042  \n",
       "2             0.005610    25  0.140248  \n",
       "3             0.001304    14  0.018260  \n",
       "4             0.001247    10  0.012473  \n",
       "5             0.001159     6  0.006952  \n",
       "6             0.000846     7  0.005920  \n",
       "7             0.000550     7  0.003852  \n",
       "8             0.000504     9  0.004536  \n",
       "9             0.000180     4  0.000719  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c730cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance matrices\n",
    "cosine_matrix_pos=create_cosine_matrix(pretrained_model,feature_importance_pos['Variable'])\n",
    "\n",
    "cosine_matrix_neg=create_cosine_matrix(pretrained_model,feature_importance_neg['Variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b776a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2000000000000002 0.7\n",
      "1.6000000000000005 0.8999999999999999\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha and delta and use optimal values to create clusters\n",
    "grid_alpha=np.arange (0.4, 4, 0.2)\n",
    "grid_delta=np.arange (0.4, 1, 0.1)\n",
    "\n",
    "alpha, delta = optimize_clusters(grid_alpha, grid_delta, feature_importance_pos, cosine_matrix_pos)\n",
    "print(alpha, delta)\n",
    "cluster_list_groups_pos, n_cluster_index_pos = calculate_clusters('top_shap', 2, delta, alpha, cosine_matrix_pos, feature_importance_pos)\n",
    "\n",
    "\n",
    "alpha, delta = optimize_clusters(grid_alpha, grid_delta, feature_importance_neg, cosine_matrix_neg)\n",
    "print(alpha, delta)\n",
    "cluster_list_groups_neg, n_cluster_index_neg = calculate_clusters('top_shap', 2, delta, alpha, cosine_matrix_neg, feature_importance_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5712ffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# total number of clusters for pos/neg\n",
    "\n",
    "print(n_cluster_index_pos)\n",
    "print(n_cluster_index_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65018b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:\n",
      "[['good', 'well', 'hope'], ['feel', 'trust', 'self', 'god']]\n",
      "negative:\n",
      "[['contribute', 'contribution', 'benefit'], ['group', 'participant', 'person', 'people', 'thing']]\n"
     ]
    }
   ],
   "source": [
    "# print top (as specified in calculate_clusters function above) clusters\n",
    "print('positive:')\n",
    "print(cluster_list_groups_pos)\n",
    "print('negative:')\n",
    "print(cluster_list_groups_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c2089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d61e23e1",
   "metadata": {},
   "source": [
    "The rest of the notebook computes different validations of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56e7fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality positive:\n",
      "0.23724499490294076\n",
      "quality negative:\n",
      "0.3458997356469264\n"
     ]
    }
   ],
   "source": [
    "# calcluate quality for top clusters\n",
    "\n",
    "quality_pos=calculate_quality_all(cluster_list_groups_pos, cosine_matrix_pos)\n",
    "print('quality positive:')\n",
    "print(quality_pos)\n",
    "\n",
    "\n",
    "quality_neg=calculate_quality_all(cluster_list_groups_neg, cosine_matrix_neg)\n",
    "print('quality negative:')\n",
    "print(quality_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ffaef51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic pos 1</th>\n",
       "      <th>topic pos 2</th>\n",
       "      <th>topic neg 1</th>\n",
       "      <th>topic neg 2</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>248.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274194</td>\n",
       "      <td>0.213710</td>\n",
       "      <td>0.326613</td>\n",
       "      <td>0.395161</td>\n",
       "      <td>0.479839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.447009</td>\n",
       "      <td>0.410753</td>\n",
       "      <td>0.469923</td>\n",
       "      <td>0.489874</td>\n",
       "      <td>0.500604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic pos 1  topic pos 2  topic neg 1  topic neg 2     outcome\n",
       "count   248.000000   248.000000   248.000000   248.000000  248.000000\n",
       "mean      0.274194     0.213710     0.326613     0.395161    0.479839\n",
       "std       0.447009     0.410753     0.469923     0.489874    0.500604\n",
       "min       0.000000     0.000000     0.000000     0.000000    0.000000\n",
       "25%       0.000000     0.000000     0.000000     0.000000    0.000000\n",
       "50%       0.000000     0.000000     0.000000     0.000000    0.000000\n",
       "75%       1.000000     0.000000     1.000000     1.000000    1.000000\n",
       "max       1.000000     1.000000     1.000000     1.000000    1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variables for each sublist in cluster_list_groups_pos\n",
    "for i, sublist in enumerate(cluster_list_groups_pos, start=1):\n",
    "    var_name = f'topic pos {i}'\n",
    "    globals()[var_name] = text_lemma.apply(lambda x: any(word in x for word in sublist)).astype(int)\n",
    "    \n",
    "for i, sublist in enumerate(cluster_list_groups_neg, start=1):\n",
    "    var_name = f'topic neg {i}'\n",
    "    # Create a dummy variable for each sublist\n",
    "    globals()[var_name] = text_lemma.apply(lambda x: any(word in x for word in sublist)).astype(int)\n",
    "\n",
    "# Now you can access the variables like topic_pos_1, topic_pos_2, etc.\n",
    "\n",
    "# Create a DataFrame with the topic dummies and the variable \"outcome\"\n",
    "df_reg = pd.DataFrame()\n",
    "for i in range(1, len(cluster_list_groups_pos) + 1):\n",
    "    df_reg[f'topic pos {i}'] = globals()[f'topic pos {i}']\n",
    "for i in range(1, len(cluster_list_groups_neg) + 1):\n",
    "    df_reg[f'topic neg {i}'] = globals()[f'topic neg {i}']\n",
    "df_reg['outcome'] = outcome\n",
    "\n",
    "df_reg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff060e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable for each topic with the total shap value for all words in topic per answer\n",
    "\n",
    "for i, sublist in enumerate(cluster_list_groups_pos, start=1):\n",
    "    var_name = f'topic pos {i}'\n",
    "    globals()[var_name] = shap_values_matrix[sublist].sum(axis=1)\n",
    "    \n",
    "for i, sublist in enumerate(cluster_list_groups_neg, start=1):\n",
    "    var_name = f'topic neg {i}'\n",
    "    globals()[var_name] = shap_values_matrix[sublist].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b31c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the topic weights and the variable \"outcome\"\n",
    "df_reg = pd.DataFrame()\n",
    "for i in range(1, len(cluster_list_groups_pos) + 1):\n",
    "    df_reg[f'Intuition Topic {i}'] = globals()[f'topic pos {i}']\n",
    "for i in range(1, len(cluster_list_groups_neg) + 1):\n",
    "    df_reg[f'Reflection Topic {i}'] = globals()[f'topic neg {i}']*(-1)\n",
    "df_reg['outcome'] = outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "347e4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to numeric data type\n",
    "df_reg = df_reg.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Store coefficients and confidence intervals\n",
    "coefficients = []\n",
    "conf_intervals = []\n",
    "\n",
    "# Run a linear regression for each topic dummy\n",
    "for column in df_reg.columns:\n",
    "    if column != 'outcome':\n",
    "        X = df_reg['outcome']\n",
    "        y = df_reg[column]\n",
    "        X = sm.add_constant(X)  # adding a constant\n",
    "\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        coefficients.append(model.params[1])\n",
    "        conf_intervals.append(model.conf_int(alpha=0.1).iloc[1].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f98a550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot coefficients with confidence intervals\n",
    "# Define your desired font size and marker size\n",
    "font_size = 18\n",
    "marker_size = 10\n",
    "line_width = 2\n",
    "\n",
    "# Plot coefficients with confidence intervals\n",
    "plt.figure(figsize=(10, 5))\n",
    "topic_order=np.arange(len(coefficients),0,-1)-1\n",
    "# Plot error bars with increased marker size and line width\n",
    "plt.errorbar(coefficients, topic_order, \n",
    "             xerr=[abs((top-bot)/2) for top, bot in conf_intervals],\n",
    "             fmt='o', markersize=marker_size, elinewidth=line_width)\n",
    "\n",
    "# Increase font size for y-axis labels\n",
    "plt.yticks(topic_order, df_reg.columns[df_reg.columns != 'outcome'], fontsize=font_size)\n",
    "\n",
    "# Get the current limits of the x-axis\n",
    "xmin, xmax = plt.xlim()\n",
    "\n",
    "# Define x-axis labels and their positions\n",
    "labels = ['Reflection', u'\\u2190 Correlation \\u2192', 'Intuition']  # Unicode characters for left and right arrows\n",
    "positions = [xmin, (xmin + xmax) / 2, xmax]  # Place 'Correlation' at the middle of the x-axis\n",
    "\n",
    "# Add new x-axis labels\n",
    "for pos, label in zip(positions, labels):\n",
    "    plt.text(pos, -0.15, label, fontsize=font_size, ha='center', va='top', transform=plt.gca().get_xaxis_transform())\n",
    "\n",
    "# Increase font size for original x-axis labels\n",
    "plt.xticks(fontsize=font_size)\n",
    "\n",
    "plt.axvline(x=0, color='black', linewidth=1)\n",
    "\n",
    "\n",
    "plt.savefig(path_graphs+'cta_topic_treatment_tdm.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08459f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quality for all topics\n",
    "cluster_list_groups_all=cluster_list_groups_pos+cluster_list_groups_neg\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_words_cluster_list_groups_all = [word for sublist in cluster_list_groups_all for word in sublist]\n",
    "# Remove duplicates by converting the list to a set\n",
    "unique_words = set(all_words_cluster_list_groups_all)\n",
    "\n",
    "# Convert the set back to a list\n",
    "all_words_cluster_list_groups_all = list(unique_words)\n",
    "\n",
    "cosine_matrix_alltopics = create_cosine_matrix(pretrained_model, all_words_cluster_list_groups_all)\n",
    "round(calculate_quality_all(cluster_list_groups_all, cosine_matrix_alltopics),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c064f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
