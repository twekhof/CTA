{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607ebe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTA_STEPS 0, 1, and 2\n",
    "# Step 0: Data Cleaning\n",
    "# Step 1: Predict outcome\n",
    "# Step 2: Compute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5daaa557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import nltk\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "from scipy.stats import sem\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d97264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57b07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4502f4",
   "metadata": {},
   "source": [
    "The dataset for the following example was also used by Roberts et al. (2014) to validate the Structural Topic Model (STM), and it is available on Harvard Dataverse. It is called \"gadarian_metadata.csv\" and is available under https://doi.org/10.7910/DVN/29405."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf51dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>age</th>\n",
       "      <th>belief.in.god</th>\n",
       "      <th>condition_number</th>\n",
       "      <th>contribution</th>\n",
       "      <th>correct.crt.answers</th>\n",
       "      <th>country.of.residence..232.us.</th>\n",
       "      <th>datelastaction</th>\n",
       "      <th>...</th>\n",
       "      <th>social.risk.taking</th>\n",
       "      <th>strategy.description</th>\n",
       "      <th>time.delay</th>\n",
       "      <th>time.pressure</th>\n",
       "      <th>time.reading.instructions</th>\n",
       "      <th>time_pred</th>\n",
       "      <th>txtorg_id</th>\n",
       "      <th>us.resident</th>\n",
       "      <th>what.cont.maximizes.group.payoff</th>\n",
       "      <th>what.cont.maximizes.your.payoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>4/26/12 3:56</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>basically, given the choices of how much to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>na</td>\n",
       "      <td>4cf9192e-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>4/27/12 23:15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>i felt like it was the right decision. i had a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfa2db4-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>4/26/12 22:48</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>don't believe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfaf9ec-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>4/24/12 12:39</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>main reason for my contribution was the the fe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfb1634-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>4/27/12 22:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>because i want to take a risk.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>na</td>\n",
       "      <td>4cfb3dd0-7591-11e2-89cc-88532e617cea</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  name path Unnamed: 2 age belief.in.god condition_number contribution  \\\n",
       "0  NaN  NaN        154  22             1               10           40   \n",
       "1  NaN  NaN        155  24             1               10           30   \n",
       "2  NaN  NaN        157  25             1               10           40   \n",
       "3  NaN  NaN        158  24             9               10           40   \n",
       "4  NaN  NaN        159  26            10               10            0   \n",
       "\n",
       "  correct.crt.answers country.of.residence..232.us. datelastaction  ...  \\\n",
       "0                   3                           232   4/26/12 3:56  ...   \n",
       "1                   0                           232  4/27/12 23:15  ...   \n",
       "2                   3                           232  4/26/12 22:48  ...   \n",
       "3                   1                           203  4/24/12 12:39  ...   \n",
       "4                   2                           129  4/27/12 22:51  ...   \n",
       "\n",
       "  social.risk.taking                               strategy.description  \\\n",
       "0                3.6  basically, given the choices of how much to co...   \n",
       "1                3.2  i felt like it was the right decision. i had a...   \n",
       "2                3.4                                      don't believe   \n",
       "3                2.9  main reason for my contribution was the the fe...   \n",
       "4                2.1                     because i want to take a risk.   \n",
       "\n",
       "  time.delay time.pressure time.reading.instructions time_pred  \\\n",
       "0          0             0                        71        na   \n",
       "1          0             0                        72        na   \n",
       "2          0             0                        36        na   \n",
       "3          0             0                       104        na   \n",
       "4          0             0                       165        na   \n",
       "\n",
       "                              txtorg_id us.resident  \\\n",
       "0  4cf9192e-7591-11e2-89cc-88532e617cea           1   \n",
       "1  4cfa2db4-7591-11e2-89cc-88532e617cea           1   \n",
       "2  4cfaf9ec-7591-11e2-89cc-88532e617cea           1   \n",
       "3  4cfb1634-7591-11e2-89cc-88532e617cea           0   \n",
       "4  4cfb3dd0-7591-11e2-89cc-88532e617cea           0   \n",
       "\n",
       "  what.cont.maximizes.group.payoff what.cont.maximizes.your.payoff  \n",
       "0                               40                              40  \n",
       "1                                0                               0  \n",
       "2                               40                              40  \n",
       "3                               40                               0  \n",
       "4                               40                               0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "path_data=\"\"\n",
    "df = pd.read_csv(path_data+\"tdm1011_metadata.csv\",sep=\",\",dtype=str)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f2c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with ideology (-4)\n",
    "# identify text and ourcome\n",
    "df['treatment']=pd.to_numeric(df['primed.for.intuition'])\n",
    "#text=df['mii_1']\n",
    "text=df['strategy.description']\n",
    "#outcome=df['ideology']-1\n",
    "outcome=df['treatment']\n",
    "text_original=df['strategy.description']\n",
    "metadata=df[['education','female','contribution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d15b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words_orig(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(str(sentence).split())\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "# Load the spacy model (for other languages, use another spacy model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = text.apply(lambda x: ' '.join([token.text for token in nlp(x) if not token.is_stop]))\n",
    "\n",
    "# lemmatize text (optional)\n",
    "\n",
    "text_lemma = text.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "df_tosave=pd.DataFrame({'text':text_lemma,'treatment':outcome, 'text_raw':text, 'text_original':text_original})\n",
    "df_tosave = pd.concat([df_tosave, metadata], axis=1)\n",
    "df_tosave = df_tosave[df_tosave['text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "text_lemma=df_tosave['text']\n",
    "outcome=df_tosave['treatment']\n",
    "text_orig=df_tosave['text_raw']\n",
    "\n",
    "\n",
    "words = text_lemma.apply(nltk.word_tokenize)\n",
    "words_orig = text_orig.apply(nltk.word_tokenize)\n",
    "\n",
    "# Flatten the list of words\n",
    "words = [word for sublist in words for word in sublist]\n",
    "words_orig = [word for sublist in words_orig for word in sublist]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist = nltk.FreqDist(words)\n",
    "\n",
    "sorted_freqdist = sorted(freqdist.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Create a frequency distribution\n",
    "freqdist_orig = nltk.FreqDist(words_orig)\n",
    "\n",
    "sorted_freqdist_orig = sorted(freqdist_orig.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# Define minimum number of characters\n",
    "min_characters = 3\n",
    "\n",
    "# Assuming text_lemma is your lemmatized text\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern=r'\\b\\w{%d,}\\b' % min_characters)\n",
    "\n",
    "# fit the vectorizer on the lemmatized text\n",
    "X = vectorizer.fit_transform(text_lemma)\n",
    "\n",
    "# X is a sparse matrix representing the bag-of-words model\n",
    "# To get the feature names (words), you can use\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# To convert the matrix into a DataFrame:\n",
    "# Here, feature_names (which are your words) are used as column names in the DataFrame\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "data_words = list(sent_to_words_orig(text_lemma))\n",
    "\n",
    "data_words_orig = list(sent_to_words_orig(text_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdb79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35a48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecdba918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: predict outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb2bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text and outcome for BERT\n",
    "\n",
    "text_for_bert=df_tosave['text']\n",
    "labels_for_bert=df_tosave['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d69a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n"
     ]
    }
   ],
   "source": [
    "# Predict outcome with BERT\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can use any seed you want\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Assume you have a DataFrame 'df' with 'text_lemma' as text column and 'outcome' as label\n",
    "texts = text_for_bert.tolist()\n",
    "labels = pd.to_numeric(labels_for_bert).tolist()\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "# Choose an optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(20):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions for each batch\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53dbb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: obtain feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to obtain feature importance\n",
    "\n",
    "# Define a prediction function, use CPU\n",
    "def f(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=512, truncation=True) for v in x])\n",
    "    outputs = model(tv)[0].detach().cpu().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# Explain the model's predictions on text\n",
    "shap_values = explainer(text_for_bert, fixed_context=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify words that could be allocated to an embedding (important for Step 3)\n",
    "unique_words = list(set(words))\n",
    "unique_words_filtered = [word for word in unique_words if len(word) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "059dc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 444)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe out of the SHAP values lists\n",
    "\n",
    "shap_allwords=pd.DataFrame(np.zeros((df_tosave.shape[0], len(unique_words_filtered))))\n",
    "shap_allwords.columns=unique_words_filtered\n",
    "\n",
    "index=0\n",
    "for i in range(text_lemma.shape[0]):\n",
    "    words_tmp=shap_values[i].data\n",
    "    shap_tmp=shap_values[i].values\n",
    "    for j in range(len(words_tmp)):\n",
    "        word_tmp=words_tmp[j].lstrip()\n",
    "        if word_tmp in unique_words_filtered:\n",
    "            shap_allwords[word_tmp][i]=shap_tmp[j]\n",
    "            index=index+1\n",
    "#             print(index)\n",
    "shap_allwords.shape      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60bba040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save SHAP values\n",
    "\n",
    "shap_allwords.to_csv(\"tdm_intuition_shap_bert_allwords_220324_train_lemma_shap_lemma.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc585c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
